{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "695a45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a6f395",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "463f5f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define the maximum sequence length and vocabulary size\n",
      "Convert the text to a sequence of integer IDs\n",
      "Create input/output pairs for the language model\n",
      "Pad the input sequences to a fixed length\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 29.5 GiB for an array with shape (19993135, 396) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPad the input sequences to a fixed length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m max_input_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m input_seqs)\n\u001b[1;32m---> 33\u001b[0m input_seqs \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_input_length\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\data_utils.py:1066\u001b[0m, in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dtype_str:\n\u001b[0;32m   1060\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1061\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`dtype` \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not compatible with `value`\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms type: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1062\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYou should set `dtype=object` for variable length \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1063\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1064\u001b[0m     )\n\u001b[1;32m-> 1066\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sequences):\n\u001b[0;32m   1068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\numeric.py:344\u001b[0m, in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order, like)\u001b[0m\n\u001b[0;32m    342\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m asarray(fill_value)\n\u001b[0;32m    343\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m fill_value\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m--> 344\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m multiarray\u001b[38;5;241m.\u001b[39mcopyto(a, fill_value, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 29.5 GiB for an array with shape (19993135, 396) and data type int32"
     ]
    }
   ],
   "source": [
    "# Load your text data\n",
    "with open('movie.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    text_data = f.readlines()\n",
    "\n",
    "text_data = [line.strip() for line in text_data]\n",
    "text_data = [line + ' eol' for line in text_data]\n",
    "\n",
    "# Define the maximum sequence length and vocabulary size\n",
    "print(\"Define the maximum sequence length and vocabulary size\")\n",
    "max_seq_length = 100  # for example\n",
    "vocab_size = 3000  # for example\n",
    "\n",
    "# Convert the text to a sequence of integer IDs\n",
    "print(\"Convert the text to a sequence of integer IDs\")\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "seqs = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Create input/output pairs for the language model\n",
    "print(\"Create input/output pairs for the language model\")\n",
    "input_seqs = []\n",
    "output_seqs = []\n",
    "for seq in seqs:\n",
    "    for i in range(1, len(seq)):\n",
    "        input_seq = seq[:i]\n",
    "        output_seq = seq[i]\n",
    "        input_seqs.append(input_seq)\n",
    "        output_seqs.append(output_seq)\n",
    "\n",
    "# Pad the input sequences to a fixed length\n",
    "print(\"Pad the input sequences to a fixed length\")\n",
    "max_input_length = max(len(seq) for seq in input_seqs)\n",
    "input_seqs = pad_sequences(input_seqs, maxlen=max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fea12c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8187bd04",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c733f1c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define the RNN language model architecture\n",
      "Train the language model\n",
      "Epoch 1/5\n",
      "Batch 1/13946 - loss: 8.0062 - acc: 0.0000\n",
      "Batch 201/13946 - loss: 5.7960 - acc: 0.1305\n",
      "Batch 401/13946 - loss: 5.6860 - acc: 0.1383\n",
      "Batch 601/13946 - loss: 5.4640 - acc: 0.1469\n",
      "Batch 801/13946 - loss: 5.3639 - acc: 0.1531\n",
      "Batch 1001/13946 - loss: 5.3963 - acc: 0.1633\n",
      "Batch 1201/13946 - loss: 5.0524 - acc: 0.1875\n",
      "Batch 1401/13946 - loss: 5.1272 - acc: 0.1750\n",
      "Batch 1601/13946 - loss: 5.0117 - acc: 0.1914\n",
      "Batch 1801/13946 - loss: 4.9222 - acc: 0.1898\n",
      "Batch 2001/13946 - loss: 4.9589 - acc: 0.1898\n",
      "Batch 2201/13946 - loss: 4.8706 - acc: 0.1813\n",
      "Batch 2401/13946 - loss: 4.7460 - acc: 0.2055\n",
      "Batch 2601/13946 - loss: 4.9985 - acc: 0.1664\n",
      "Batch 2801/13946 - loss: 4.7661 - acc: 0.1844\n",
      "Batch 3001/13946 - loss: 4.8235 - acc: 0.1984\n",
      "Batch 3201/13946 - loss: 4.7286 - acc: 0.1977\n",
      "Batch 3401/13946 - loss: 4.5682 - acc: 0.2250\n",
      "Batch 3601/13946 - loss: 4.5681 - acc: 0.2156\n",
      "Batch 3801/13946 - loss: 4.6953 - acc: 0.2125\n",
      "Batch 4001/13946 - loss: 4.8686 - acc: 0.1937\n",
      "Batch 4201/13946 - loss: 4.6803 - acc: 0.1977\n",
      "Batch 4401/13946 - loss: 4.7146 - acc: 0.2039\n",
      "Batch 4601/13946 - loss: 4.7357 - acc: 0.2125\n",
      "Batch 4801/13946 - loss: 4.7942 - acc: 0.1875\n",
      "Batch 5001/13946 - loss: 4.8364 - acc: 0.1930\n",
      "Batch 5201/13946 - loss: 4.6030 - acc: 0.2062\n",
      "Batch 5401/13946 - loss: 4.4876 - acc: 0.2281\n",
      "Batch 5601/13946 - loss: 4.7423 - acc: 0.1930\n",
      "Batch 5801/13946 - loss: 4.6364 - acc: 0.2055\n",
      "Batch 6001/13946 - loss: 4.6694 - acc: 0.2062\n",
      "Batch 6201/13946 - loss: 4.6051 - acc: 0.2047\n",
      "Batch 6401/13946 - loss: 4.5660 - acc: 0.2211\n",
      "Batch 6601/13946 - loss: 4.5630 - acc: 0.1992\n",
      "Batch 6801/13946 - loss: 4.6339 - acc: 0.2148\n",
      "Batch 7001/13946 - loss: 4.6311 - acc: 0.2133\n",
      "Batch 7201/13946 - loss: 4.6630 - acc: 0.2141\n",
      "Batch 7401/13946 - loss: 4.6742 - acc: 0.2133\n",
      "Batch 7601/13946 - loss: 4.5639 - acc: 0.2492\n",
      "Batch 7801/13946 - loss: 4.7712 - acc: 0.2242\n",
      "Batch 8001/13946 - loss: 4.6110 - acc: 0.2188\n",
      "Batch 8201/13946 - loss: 4.6896 - acc: 0.2055\n",
      "Batch 8401/13946 - loss: 4.6299 - acc: 0.2180\n",
      "Batch 8601/13946 - loss: 4.6509 - acc: 0.2133\n",
      "Batch 8801/13946 - loss: 4.5907 - acc: 0.2227\n",
      "Batch 9001/13946 - loss: 4.5566 - acc: 0.2414\n",
      "Batch 9201/13946 - loss: 4.6387 - acc: 0.2164\n",
      "Batch 9401/13946 - loss: 4.6267 - acc: 0.2328\n",
      "Batch 9601/13946 - loss: 4.4925 - acc: 0.2375\n",
      "Batch 9801/13946 - loss: 4.6161 - acc: 0.2289\n",
      "Batch 10001/13946 - loss: 4.4669 - acc: 0.2391\n",
      "Batch 10201/13946 - loss: 4.4030 - acc: 0.2469\n",
      "Batch 10401/13946 - loss: 4.5175 - acc: 0.2313\n",
      "Batch 10601/13946 - loss: 4.5077 - acc: 0.2328\n",
      "Batch 10801/13946 - loss: 4.5174 - acc: 0.2328\n",
      "Batch 11001/13946 - loss: 4.6064 - acc: 0.2234\n",
      "Batch 11201/13946 - loss: 4.5477 - acc: 0.2219\n",
      "Batch 11401/13946 - loss: 4.4362 - acc: 0.2406\n",
      "Batch 11601/13946 - loss: 4.6726 - acc: 0.1992\n",
      "Batch 11801/13946 - loss: 4.5090 - acc: 0.2211\n",
      "Batch 12001/13946 - loss: 4.4994 - acc: 0.2359\n",
      "Batch 12201/13946 - loss: 4.5258 - acc: 0.2094\n",
      "Batch 12401/13946 - loss: 4.5877 - acc: 0.2344\n",
      "Batch 12601/13946 - loss: 4.5008 - acc: 0.2352\n",
      "Batch 12801/13946 - loss: 4.5377 - acc: 0.2516\n",
      "Batch 13001/13946 - loss: 4.5584 - acc: 0.2125\n",
      "Batch 13201/13946 - loss: 4.5238 - acc: 0.2203\n",
      "Batch 13401/13946 - loss: 4.6190 - acc: 0.2234\n",
      "Batch 13601/13946 - loss: 4.5186 - acc: 0.2281\n",
      "Batch 13801/13946 - loss: 4.7382 - acc: 0.2211\n",
      "Epoch loss: 4.7294 - epoch acc: 0.2101\n",
      "Epoch 2/5\n",
      "Batch 1/13946 - loss: 4.7636 - acc: 0.2086\n",
      "Batch 201/13946 - loss: 4.5598 - acc: 0.2195\n",
      "Batch 401/13946 - loss: 4.4005 - acc: 0.2250\n",
      "Batch 601/13946 - loss: 4.4808 - acc: 0.2227\n",
      "Batch 801/13946 - loss: 4.5163 - acc: 0.2219\n",
      "Batch 1001/13946 - loss: 4.6367 - acc: 0.2016\n",
      "Batch 1201/13946 - loss: 4.4211 - acc: 0.2305\n",
      "Batch 1401/13946 - loss: 4.6064 - acc: 0.2031\n",
      "Batch 1601/13946 - loss: 4.5131 - acc: 0.2102\n",
      "Batch 1801/13946 - loss: 4.4247 - acc: 0.2219\n",
      "Batch 2001/13946 - loss: 4.5468 - acc: 0.2070\n",
      "Batch 2201/13946 - loss: 4.3794 - acc: 0.2125\n",
      "Batch 2401/13946 - loss: 4.3533 - acc: 0.2445\n",
      "Batch 2601/13946 - loss: 4.6647 - acc: 0.1945\n",
      "Batch 2801/13946 - loss: 4.4171 - acc: 0.2102\n",
      "Batch 3001/13946 - loss: 4.4787 - acc: 0.2180\n",
      "Batch 3201/13946 - loss: 4.4272 - acc: 0.2227\n",
      "Batch 3401/13946 - loss: 4.2986 - acc: 0.2477\n",
      "Batch 3601/13946 - loss: 4.2793 - acc: 0.2445\n",
      "Batch 3801/13946 - loss: 4.3802 - acc: 0.2391\n",
      "Batch 4001/13946 - loss: 4.6164 - acc: 0.2086\n",
      "Batch 4201/13946 - loss: 4.4306 - acc: 0.2078\n",
      "Batch 4401/13946 - loss: 4.4524 - acc: 0.2320\n",
      "Batch 4601/13946 - loss: 4.4325 - acc: 0.2281\n",
      "Batch 4801/13946 - loss: 4.5750 - acc: 0.2070\n",
      "Batch 5001/13946 - loss: 4.5950 - acc: 0.2094\n",
      "Batch 5201/13946 - loss: 4.3980 - acc: 0.2242\n",
      "Batch 5401/13946 - loss: 4.2407 - acc: 0.2531\n",
      "Batch 5601/13946 - loss: 4.5380 - acc: 0.2094\n",
      "Batch 5801/13946 - loss: 4.4716 - acc: 0.2164\n",
      "Batch 6001/13946 - loss: 4.4900 - acc: 0.2117\n",
      "Batch 6201/13946 - loss: 4.4057 - acc: 0.2180\n",
      "Batch 6401/13946 - loss: 4.3526 - acc: 0.2297\n",
      "Batch 6601/13946 - loss: 4.4041 - acc: 0.2180\n",
      "Batch 6801/13946 - loss: 4.4452 - acc: 0.2219\n",
      "Batch 7001/13946 - loss: 4.4351 - acc: 0.2211\n",
      "Batch 7201/13946 - loss: 4.4895 - acc: 0.2320\n",
      "Batch 7401/13946 - loss: 4.5124 - acc: 0.2172\n",
      "Batch 7601/13946 - loss: 4.3878 - acc: 0.2641\n",
      "Batch 7801/13946 - loss: 4.6157 - acc: 0.2305\n",
      "Batch 8001/13946 - loss: 4.4628 - acc: 0.2242\n",
      "Batch 8201/13946 - loss: 4.5394 - acc: 0.2133\n",
      "Batch 8401/13946 - loss: 4.4651 - acc: 0.2313\n",
      "Batch 8601/13946 - loss: 4.4781 - acc: 0.2328\n",
      "Batch 8801/13946 - loss: 4.4071 - acc: 0.2273\n",
      "Batch 9001/13946 - loss: 4.4120 - acc: 0.2469\n",
      "Batch 9201/13946 - loss: 4.4902 - acc: 0.2266\n",
      "Batch 9401/13946 - loss: 4.5271 - acc: 0.2328\n",
      "Batch 9601/13946 - loss: 4.3561 - acc: 0.2422\n",
      "Batch 9801/13946 - loss: 4.5105 - acc: 0.2336\n",
      "Batch 10001/13946 - loss: 4.3464 - acc: 0.2469\n",
      "Batch 10201/13946 - loss: 4.2557 - acc: 0.2531\n",
      "Batch 10401/13946 - loss: 4.3640 - acc: 0.2461\n",
      "Batch 10601/13946 - loss: 4.3757 - acc: 0.2328\n",
      "Batch 10801/13946 - loss: 4.3976 - acc: 0.2344\n",
      "Batch 11001/13946 - loss: 4.4868 - acc: 0.2227\n",
      "Batch 11201/13946 - loss: 4.4065 - acc: 0.2258\n",
      "Batch 11401/13946 - loss: 4.3024 - acc: 0.2477\n",
      "Batch 11601/13946 - loss: 4.5523 - acc: 0.2141\n",
      "Batch 11801/13946 - loss: 4.3813 - acc: 0.2273\n",
      "Batch 12001/13946 - loss: 4.4108 - acc: 0.2336\n",
      "Batch 12201/13946 - loss: 4.4149 - acc: 0.2219\n",
      "Batch 12401/13946 - loss: 4.5117 - acc: 0.2430\n",
      "Batch 12601/13946 - loss: 4.4280 - acc: 0.2438\n",
      "Batch 12801/13946 - loss: 4.4179 - acc: 0.2570\n",
      "Batch 13001/13946 - loss: 4.4285 - acc: 0.2273\n",
      "Batch 13201/13946 - loss: 4.4142 - acc: 0.2344\n",
      "Batch 13401/13946 - loss: 4.5317 - acc: 0.2211\n",
      "Batch 13601/13946 - loss: 4.4005 - acc: 0.2344\n",
      "Batch 13801/13946 - loss: 4.6407 - acc: 0.2352\n",
      "Epoch loss: 4.4546 - epoch acc: 0.2297\n",
      "Epoch 3/5\n",
      "Batch 1/13946 - loss: 4.6308 - acc: 0.2188\n",
      "Batch 201/13946 - loss: 4.4525 - acc: 0.2234\n",
      "Batch 401/13946 - loss: 4.3042 - acc: 0.2297\n",
      "Batch 601/13946 - loss: 4.3970 - acc: 0.2336\n",
      "Batch 801/13946 - loss: 4.4086 - acc: 0.2242\n",
      "Batch 1001/13946 - loss: 4.5380 - acc: 0.2102\n",
      "Batch 1201/13946 - loss: 4.3647 - acc: 0.2438\n",
      "Batch 1401/13946 - loss: 4.5067 - acc: 0.2125\n",
      "Batch 1601/13946 - loss: 4.4544 - acc: 0.2133\n",
      "Batch 1801/13946 - loss: 4.3669 - acc: 0.2367\n",
      "Batch 2001/13946 - loss: 4.4650 - acc: 0.2234\n",
      "Batch 2201/13946 - loss: 4.2887 - acc: 0.2195\n",
      "Batch 2401/13946 - loss: 4.2689 - acc: 0.2500\n",
      "Batch 2601/13946 - loss: 4.5846 - acc: 0.2023\n",
      "Batch 2801/13946 - loss: 4.3189 - acc: 0.2234\n",
      "Batch 3001/13946 - loss: 4.3816 - acc: 0.2266\n",
      "Batch 3201/13946 - loss: 4.3193 - acc: 0.2266\n",
      "Batch 3401/13946 - loss: 4.2231 - acc: 0.2508\n",
      "Batch 3601/13946 - loss: 4.1781 - acc: 0.2594\n",
      "Batch 3801/13946 - loss: 4.3184 - acc: 0.2477\n",
      "Batch 4001/13946 - loss: 4.5294 - acc: 0.2188\n",
      "Batch 4201/13946 - loss: 4.3523 - acc: 0.2211\n",
      "Batch 4401/13946 - loss: 4.3922 - acc: 0.2313\n",
      "Batch 4601/13946 - loss: 4.3704 - acc: 0.2281\n",
      "Batch 4801/13946 - loss: 4.4725 - acc: 0.2141\n",
      "Batch 5001/13946 - loss: 4.5074 - acc: 0.2141\n",
      "Batch 5201/13946 - loss: 4.3186 - acc: 0.2453\n",
      "Batch 5401/13946 - loss: 4.1870 - acc: 0.2555\n",
      "Batch 5601/13946 - loss: 4.4520 - acc: 0.2156\n",
      "Batch 5801/13946 - loss: 4.4216 - acc: 0.2227\n",
      "Batch 6001/13946 - loss: 4.4166 - acc: 0.2164\n",
      "Batch 6201/13946 - loss: 4.3413 - acc: 0.2320\n",
      "Batch 6401/13946 - loss: 4.2771 - acc: 0.2469\n",
      "Batch 6601/13946 - loss: 4.3397 - acc: 0.2305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6801/13946 - loss: 4.3370 - acc: 0.2305\n",
      "Batch 7001/13946 - loss: 4.3541 - acc: 0.2266\n",
      "Batch 7201/13946 - loss: 4.4164 - acc: 0.2352\n",
      "Batch 7401/13946 - loss: 4.4323 - acc: 0.2336\n",
      "Batch 7601/13946 - loss: 4.3352 - acc: 0.2719\n",
      "Batch 7801/13946 - loss: 4.5367 - acc: 0.2227\n",
      "Batch 8001/13946 - loss: 4.3668 - acc: 0.2461\n",
      "Batch 8201/13946 - loss: 4.4924 - acc: 0.2211\n",
      "Batch 8401/13946 - loss: 4.3972 - acc: 0.2438\n",
      "Batch 8601/13946 - loss: 4.4098 - acc: 0.2430\n",
      "Batch 8801/13946 - loss: 4.3637 - acc: 0.2336\n",
      "Batch 9001/13946 - loss: 4.3430 - acc: 0.2578\n",
      "Batch 9201/13946 - loss: 4.4216 - acc: 0.2313\n",
      "Batch 9401/13946 - loss: 4.4691 - acc: 0.2352\n",
      "Batch 9601/13946 - loss: 4.2985 - acc: 0.2523\n",
      "Batch 9801/13946 - loss: 4.4496 - acc: 0.2398\n",
      "Batch 10001/13946 - loss: 4.2688 - acc: 0.2641\n",
      "Batch 10201/13946 - loss: 4.1860 - acc: 0.2602\n",
      "Batch 10401/13946 - loss: 4.3021 - acc: 0.2438\n",
      "Batch 10601/13946 - loss: 4.3292 - acc: 0.2422\n",
      "Batch 10801/13946 - loss: 4.3419 - acc: 0.2422\n",
      "Batch 11001/13946 - loss: 4.4183 - acc: 0.2250\n",
      "Batch 11201/13946 - loss: 4.3599 - acc: 0.2359\n",
      "Batch 11401/13946 - loss: 4.2455 - acc: 0.2578\n",
      "Batch 11601/13946 - loss: 4.5448 - acc: 0.2102\n",
      "Batch 11801/13946 - loss: 4.3245 - acc: 0.2391\n",
      "Batch 12001/13946 - loss: 4.3337 - acc: 0.2430\n",
      "Batch 12201/13946 - loss: 4.3814 - acc: 0.2258\n",
      "Batch 12401/13946 - loss: 4.4654 - acc: 0.2445\n",
      "Batch 12601/13946 - loss: 4.4034 - acc: 0.2492\n",
      "Batch 12801/13946 - loss: 4.3730 - acc: 0.2516\n",
      "Batch 13001/13946 - loss: 4.3713 - acc: 0.2297\n",
      "Batch 13201/13946 - loss: 4.3602 - acc: 0.2445\n",
      "Batch 13401/13946 - loss: 4.4862 - acc: 0.2258\n",
      "Batch 13601/13946 - loss: 4.3491 - acc: 0.2406\n",
      "Batch 13801/13946 - loss: 4.5991 - acc: 0.2313\n",
      "Epoch loss: 4.3848 - epoch acc: 0.2354\n",
      "Epoch 4/5\n",
      "Batch 1/13946 - loss: 4.5677 - acc: 0.2164\n",
      "Batch 201/13946 - loss: 4.4337 - acc: 0.2383\n",
      "Batch 401/13946 - loss: 4.2487 - acc: 0.2391\n",
      "Batch 601/13946 - loss: 4.3356 - acc: 0.2305\n",
      "Batch 801/13946 - loss: 4.3550 - acc: 0.2383\n",
      "Batch 1001/13946 - loss: 4.4976 - acc: 0.2102\n",
      "Batch 1201/13946 - loss: 4.3261 - acc: 0.2383\n",
      "Batch 1401/13946 - loss: 4.4968 - acc: 0.2109\n",
      "Batch 1601/13946 - loss: 4.4136 - acc: 0.2281\n",
      "Batch 1801/13946 - loss: 4.3225 - acc: 0.2445\n",
      "Batch 2001/13946 - loss: 4.4060 - acc: 0.2203\n",
      "Batch 2201/13946 - loss: 4.2202 - acc: 0.2195\n",
      "Batch 2401/13946 - loss: 4.2058 - acc: 0.2609\n",
      "Batch 2601/13946 - loss: 4.5361 - acc: 0.2094\n",
      "Batch 2801/13946 - loss: 4.2852 - acc: 0.2203\n",
      "Batch 3001/13946 - loss: 4.3300 - acc: 0.2328\n",
      "Batch 3201/13946 - loss: 4.2945 - acc: 0.2359\n",
      "Batch 3401/13946 - loss: 4.1912 - acc: 0.2547\n",
      "Batch 3601/13946 - loss: 4.1222 - acc: 0.2477\n",
      "Batch 3801/13946 - loss: 4.2751 - acc: 0.2477\n",
      "Batch 4001/13946 - loss: 4.4732 - acc: 0.2172\n",
      "Batch 4201/13946 - loss: 4.3139 - acc: 0.2297\n",
      "Batch 4401/13946 - loss: 4.3578 - acc: 0.2344\n",
      "Batch 4601/13946 - loss: 4.3674 - acc: 0.2313\n",
      "Batch 4801/13946 - loss: 4.4514 - acc: 0.2148\n",
      "Batch 5001/13946 - loss: 4.4770 - acc: 0.2141\n",
      "Batch 5201/13946 - loss: 4.2738 - acc: 0.2383\n",
      "Batch 5401/13946 - loss: 4.1594 - acc: 0.2586\n",
      "Batch 5601/13946 - loss: 4.3952 - acc: 0.2258\n",
      "Batch 5801/13946 - loss: 4.3679 - acc: 0.2219\n",
      "Batch 6001/13946 - loss: 4.3858 - acc: 0.2180\n",
      "Batch 6201/13946 - loss: 4.2883 - acc: 0.2281\n",
      "Batch 6401/13946 - loss: 4.2460 - acc: 0.2375\n",
      "Batch 6601/13946 - loss: 4.2929 - acc: 0.2398\n",
      "Batch 6801/13946 - loss: 4.3295 - acc: 0.2406\n",
      "Batch 7001/13946 - loss: 4.3213 - acc: 0.2313\n",
      "Batch 7201/13946 - loss: 4.3534 - acc: 0.2422\n",
      "Batch 7401/13946 - loss: 4.3793 - acc: 0.2391\n",
      "Batch 7601/13946 - loss: 4.2584 - acc: 0.2703\n",
      "Batch 7801/13946 - loss: 4.5377 - acc: 0.2305\n",
      "Batch 8001/13946 - loss: 4.3484 - acc: 0.2359\n",
      "Batch 8201/13946 - loss: 4.4384 - acc: 0.2211\n",
      "Batch 8401/13946 - loss: 4.3464 - acc: 0.2352\n",
      "Batch 8601/13946 - loss: 4.3508 - acc: 0.2414\n",
      "Batch 8801/13946 - loss: 4.3038 - acc: 0.2422\n",
      "Batch 9001/13946 - loss: 4.3060 - acc: 0.2633\n",
      "Batch 9201/13946 - loss: 4.4004 - acc: 0.2344\n",
      "Batch 9401/13946 - loss: 4.4206 - acc: 0.2414\n",
      "Batch 9601/13946 - loss: 4.2654 - acc: 0.2484\n",
      "Batch 9801/13946 - loss: 4.4176 - acc: 0.2398\n",
      "Batch 10001/13946 - loss: 4.2139 - acc: 0.2625\n",
      "Batch 10201/13946 - loss: 4.1334 - acc: 0.2680\n",
      "Batch 10401/13946 - loss: 4.2743 - acc: 0.2508\n",
      "Batch 10601/13946 - loss: 4.3048 - acc: 0.2508\n",
      "Batch 10801/13946 - loss: 4.3187 - acc: 0.2398\n",
      "Batch 11001/13946 - loss: 4.3716 - acc: 0.2289\n",
      "Batch 11201/13946 - loss: 4.3346 - acc: 0.2352\n",
      "Batch 11401/13946 - loss: 4.2166 - acc: 0.2625\n",
      "Batch 11601/13946 - loss: 4.5054 - acc: 0.2141\n",
      "Batch 11801/13946 - loss: 4.2794 - acc: 0.2383\n",
      "Batch 12001/13946 - loss: 4.2867 - acc: 0.2617\n",
      "Batch 12201/13946 - loss: 4.3430 - acc: 0.2344\n",
      "Batch 12401/13946 - loss: 4.4423 - acc: 0.2500\n",
      "Batch 12601/13946 - loss: 4.3482 - acc: 0.2555\n",
      "Batch 12801/13946 - loss: 4.3498 - acc: 0.2445\n",
      "Batch 13001/13946 - loss: 4.3535 - acc: 0.2375\n",
      "Batch 13201/13946 - loss: 4.3116 - acc: 0.2414\n",
      "Batch 13401/13946 - loss: 4.4756 - acc: 0.2266\n",
      "Batch 13601/13946 - loss: 4.3021 - acc: 0.2477\n",
      "Batch 13801/13946 - loss: 4.5550 - acc: 0.2375\n",
      "Epoch loss: 4.3463 - epoch acc: 0.2386\n",
      "Epoch 5/5\n",
      "Batch 1/13946 - loss: 4.5562 - acc: 0.2180\n",
      "Batch 201/13946 - loss: 4.3593 - acc: 0.2305\n",
      "Batch 401/13946 - loss: 4.2408 - acc: 0.2391\n",
      "Batch 601/13946 - loss: 4.3196 - acc: 0.2313\n",
      "Batch 801/13946 - loss: 4.3146 - acc: 0.2430\n",
      "Batch 1001/13946 - loss: 4.4626 - acc: 0.2180\n",
      "Batch 1201/13946 - loss: 4.2718 - acc: 0.2406\n",
      "Batch 1401/13946 - loss: 4.4542 - acc: 0.2133\n",
      "Batch 1601/13946 - loss: 4.3642 - acc: 0.2258\n",
      "Batch 1801/13946 - loss: 4.2552 - acc: 0.2445\n",
      "Batch 2001/13946 - loss: 4.3852 - acc: 0.2305\n",
      "Batch 2201/13946 - loss: 4.1871 - acc: 0.2289\n",
      "Batch 2401/13946 - loss: 4.1332 - acc: 0.2633\n",
      "Batch 2601/13946 - loss: 4.4761 - acc: 0.2070\n",
      "Batch 2801/13946 - loss: 4.2809 - acc: 0.2281\n",
      "Batch 3001/13946 - loss: 4.3543 - acc: 0.2250\n",
      "Batch 3201/13946 - loss: 4.2518 - acc: 0.2477\n",
      "Batch 3401/13946 - loss: 4.1383 - acc: 0.2547\n",
      "Batch 3601/13946 - loss: 4.1208 - acc: 0.2578\n",
      "Batch 3801/13946 - loss: 4.2516 - acc: 0.2570\n",
      "Batch 4001/13946 - loss: 4.4646 - acc: 0.2234\n",
      "Batch 4201/13946 - loss: 4.3050 - acc: 0.2281\n",
      "Batch 4401/13946 - loss: 4.3332 - acc: 0.2359\n",
      "Batch 4601/13946 - loss: 4.2843 - acc: 0.2383\n",
      "Batch 4801/13946 - loss: 4.4283 - acc: 0.2148\n",
      "Batch 5001/13946 - loss: 4.4848 - acc: 0.2133\n",
      "Batch 5201/13946 - loss: 4.2512 - acc: 0.2492\n",
      "Batch 5401/13946 - loss: 4.1196 - acc: 0.2617\n",
      "Batch 5601/13946 - loss: 4.3699 - acc: 0.2344\n",
      "Batch 5801/13946 - loss: 4.3741 - acc: 0.2266\n",
      "Batch 6001/13946 - loss: 4.3547 - acc: 0.2242\n",
      "Batch 6201/13946 - loss: 4.2556 - acc: 0.2289\n",
      "Batch 6401/13946 - loss: 4.2153 - acc: 0.2461\n",
      "Batch 6601/13946 - loss: 4.3077 - acc: 0.2297\n",
      "Batch 6801/13946 - loss: 4.3234 - acc: 0.2352\n",
      "Batch 7001/13946 - loss: 4.3066 - acc: 0.2359\n",
      "Batch 7201/13946 - loss: 4.3470 - acc: 0.2320\n",
      "Batch 7401/13946 - loss: 4.3740 - acc: 0.2477\n",
      "Batch 7601/13946 - loss: 4.3043 - acc: 0.2711\n",
      "Batch 7801/13946 - loss: 4.5196 - acc: 0.2328\n",
      "Batch 8001/13946 - loss: 4.3205 - acc: 0.2414\n",
      "Batch 8201/13946 - loss: 4.4332 - acc: 0.2305\n",
      "Batch 8401/13946 - loss: 4.3233 - acc: 0.2359\n",
      "Batch 8601/13946 - loss: 4.3275 - acc: 0.2414\n",
      "Batch 8801/13946 - loss: 4.2751 - acc: 0.2453\n",
      "Batch 9001/13946 - loss: 4.2740 - acc: 0.2617\n",
      "Batch 9201/13946 - loss: 4.3665 - acc: 0.2430\n",
      "Batch 9401/13946 - loss: 4.4027 - acc: 0.2508\n",
      "Batch 9601/13946 - loss: 4.2357 - acc: 0.2602\n",
      "Batch 9801/13946 - loss: 4.4054 - acc: 0.2414\n",
      "Batch 10001/13946 - loss: 4.1914 - acc: 0.2625\n",
      "Batch 10201/13946 - loss: 4.1085 - acc: 0.2750\n",
      "Batch 10401/13946 - loss: 4.2416 - acc: 0.2539\n",
      "Batch 10601/13946 - loss: 4.2835 - acc: 0.2555\n",
      "Batch 10801/13946 - loss: 4.2791 - acc: 0.2422\n",
      "Batch 11001/13946 - loss: 4.3438 - acc: 0.2305\n",
      "Batch 11201/13946 - loss: 4.2951 - acc: 0.2383\n",
      "Batch 11401/13946 - loss: 4.2165 - acc: 0.2625\n",
      "Batch 11601/13946 - loss: 4.4928 - acc: 0.2188\n",
      "Batch 11801/13946 - loss: 4.2560 - acc: 0.2391\n",
      "Batch 12001/13946 - loss: 4.2821 - acc: 0.2547\n",
      "Batch 12201/13946 - loss: 4.3055 - acc: 0.2367\n",
      "Batch 12401/13946 - loss: 4.4388 - acc: 0.2453\n",
      "Batch 12601/13946 - loss: 4.3379 - acc: 0.2602\n",
      "Batch 12801/13946 - loss: 4.3332 - acc: 0.2477\n",
      "Batch 13001/13946 - loss: 4.3658 - acc: 0.2320\n",
      "Batch 13201/13946 - loss: 4.2680 - acc: 0.2477\n",
      "Batch 13401/13946 - loss: 4.4190 - acc: 0.2227\n",
      "Batch 13601/13946 - loss: 4.3076 - acc: 0.2523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13801/13946 - loss: 4.5652 - acc: 0.2422\n",
      "Epoch loss: 4.3209 - epoch acc: 0.2407\n",
      "Generate text using the language model\n",
      "1/1 [==============================] - 1s 798ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "The quick brown fox is at sun unknown eol eol to eat fried it\n"
     ]
    }
   ],
   "source": [
    "# Set up CPU and GPU usage\n",
    "#physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#print(physical_devices)\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# Define the RNN language model architecture\n",
    "print(\"Define the RNN language model architecture\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_input_length))\n",
    "#model.add(LSTM(100))\n",
    "model.add(LSTM(200, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "#model = tf.keras.models.load_model('model_movie.h5')\n",
    "\n",
    "# Train the language model\n",
    "print(\"Train the language model\")\n",
    "#output_seqs_categorical = tf.keras.utils.to_categorical(output_seqs, num_classes=vocab_size)\n",
    "#model.fit(input_seqs, output_seqs_categorical, epochs=10, batch_size=128)\n",
    "\n",
    "# Set the batch size and number of epochs\n",
    "batch_size = 1280\n",
    "num_epochs = 5\n",
    "\n",
    "# Calculate the number of batches\n",
    "num_batches = int(np.ceil(len(output_seqs) / batch_size))\n",
    "\n",
    "# Train the language model in batches\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(output_seqs))\n",
    "        input_batch = input_seqs[start_idx:end_idx]\n",
    "        output_batch = output_seqs[start_idx:end_idx]\n",
    "\n",
    "        # Convert output to categorical\n",
    "        output_batch_categorical = tf.keras.utils.to_categorical(output_batch, num_classes=vocab_size)\n",
    "\n",
    "        # Train the model on the current batch\n",
    "        batch_loss, batch_acc = model.train_on_batch(input_batch, output_batch_categorical)\n",
    "        epoch_losses.append(batch_loss)\n",
    "        epoch_accs.append(batch_acc)\n",
    "        if (i % 200 == 0):\n",
    "            print('Batch {}/{} - loss: {:.4f} - acc: {:.4f}'.format(i+1, num_batches, batch_loss, batch_acc))\n",
    "\n",
    "    # Calculate and print the epoch loss and accuracy\n",
    "    epoch_loss = np.mean(epoch_losses)\n",
    "    epoch_acc = np.mean(epoch_accs)\n",
    "    print('Epoch loss: {:.4f} - epoch acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "    model.save('model_literature_2.h5')\n",
    "\n",
    "model.save('model_literature_2.h5')\n",
    "\n",
    "\n",
    "# Generate text using the language model\n",
    "print(\"Generate text using the language model\")\n",
    "seed_text = \"The quick brown fox\"\n",
    "for i in range(10):\n",
    "    input_seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    input_seq = pad_sequences([input_seq], maxlen=max_input_length)\n",
    "    output_probs = model.predict(input_seq)[0]\n",
    "    next_word_id = np.random.choice(vocab_size, p=output_probs)\n",
    "    next_word = tokenizer.index_word[next_word_id]\n",
    "    seed_text += \" \" + next_word\n",
    "print(seed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c65f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_movie_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3fd45b",
   "metadata": {},
   "source": [
    "## Upload model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b187a3fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.load_model('model_movie_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af368d9",
   "metadata": {},
   "source": [
    "## Get top words for first word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c23cadbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'i', 'this', 'it', 'and', 'but', \"it's\", 'in', 'if', 'a', 'there', 'he', 'unknown', 'as', 'they', 'what', 'so', 'you', 'when', 'she', 'for', 'one', 'not', 'however', 'all', \"i'm\", 'we', 'that', 'my', 'well', 'even', 'at', 'after', 'no', 'to', 'then', 'while', 'also', 'his', 'some', 'now', \"there's\", 'why', 'how', 'with', 'just', \"don't\", 'although', 'of', \"i've\", 'oh', 'on', 'maybe', \"that's\", 'unfortunately', 'its', 'or', 'first', 'is', 'yes', 'like', 'overall', 'from', 'most', 'these', 'an', 'her', 'by', \"he's\", 'instead', 'another', 'who', 'perhaps', 'here', 'because', 'though', 'still', 'very', 'anyway', 'only', 'do', 'every', 'despite', 'yet', 'watch', 'many', 'great', \"i'd\", 'where', 'which', 'nothing', 'sure', 'both', 'once', 'ok', 'good', 'other', 'everything', 'too', 'john']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# Open the file and read in the data\n",
    "with open('movie.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Split the data into individual sentences\n",
    "sentences = data.split('\\n')\n",
    "\n",
    "# Create a dictionary to store the word frequencies\n",
    "word_freq = collections.defaultdict(int)\n",
    "\n",
    "# Loop through each sentence and count the frequency of the first word\n",
    "for sentence in sentences:\n",
    "    # Check if the sentence is empty\n",
    "    if sentence.strip():\n",
    "        first_word = sentence.split()[0]\n",
    "        word_freq[first_word] += 1\n",
    "\n",
    "# Sort the dictionary by frequency in descending order\n",
    "sorted_freq = sorted(word_freq.items(), key=lambda x: x[1] if x[0] not in [\"'\", 'eol', 'EOL'] else -1, reverse=True)\n",
    "\n",
    "# Extract the 100 most frequent words\n",
    "first_top_words = [word for word, freq in sorted_freq[:100]]\n",
    "\n",
    "# Print the keyword list\n",
    "print(first_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379328b6",
   "metadata": {},
   "source": [
    "## IDX + Ziphian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "291821f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open\n",
      "freq\n",
      "idf\n",
      "2.9386061893283184\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the text data\n",
    "with open('movie.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    text = f.readlines()\n",
    "    \n",
    "text = [line.strip() for line in text]\n",
    "text = [line + ' eol' for line in text]\n",
    "words = ' '.join(text).split()\n",
    "\n",
    "print('open')\n",
    "# Build a dictionary of document frequencies\n",
    "doc_freq = Counter(words)\n",
    "print('freq')\n",
    "# Calculate the IDF weights\n",
    "num_docs = len(words)\n",
    "idf = {}\n",
    "for word in doc_freq:\n",
    "    idf[word] = math.log(num_docs / (1 + doc_freq[word]))\n",
    "print('idf')\n",
    "# Store the IDF weights\n",
    "# You can print or use this dictionary to perform further processing or analysis\n",
    "print(idf['eol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99d7383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_ziphian_tree(output_probs, tokenizer, n):\n",
    "    index_to_prob = {idx: output_probs[idx] for idx in range(len(output_probs))}\n",
    "    word_to_prob = {tokenizer.index_word[idx + 1]: prob for idx, prob in index_to_prob.items()}\n",
    "\n",
    "    keys = word_to_prob.keys()\n",
    "    for key in keys:\n",
    "        word_to_prob[key] *= idf[key]\n",
    "    \n",
    "    # Sort the words in descending order of probability\n",
    "    sorted_word_prob = sorted(word_to_prob.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the sorted probabilities\n",
    "    sorted_probs = [p for w, p in sorted_word_prob]\n",
    "\n",
    "    # Define the value of alpha\n",
    "    alpha = 1.0\n",
    "\n",
    "    # Calculate the Zipfian distribution\n",
    "    zipf_probs = [1 / (i + 1) ** alpha for i in range(len(sorted_probs))]\n",
    "    zipf_probs = np.array(zipf_probs) / np.sum(zipf_probs)\n",
    "\n",
    "    # Apply the Zipfian distribution to the sorted probabilities\n",
    "    weighted_probs = zipf_probs * sorted_probs\n",
    "\n",
    "    # Assign the new probabilities to the original words\n",
    "    for i, (word, prob) in enumerate(sorted_word_prob):\n",
    "        word_to_prob[word] = weighted_probs[i]\n",
    "\n",
    "    sorted_cp_final = sorted(word_to_prob.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Get the top N keys\n",
    "    top_n = [sorted_cp_final[i][0] for i in range(min(n, len(sorted_cp_final)))]\n",
    "    \n",
    "    return top_n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf960b9",
   "metadata": {},
   "source": [
    "## Binary tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a93759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pbt(lst, N, input_string):\n",
    "    # build dictionary\n",
    "    dictionary = {}\n",
    "    for i in range(len(lst)):\n",
    "        bit_code = format(i, f'0{N}b')\n",
    "        dictionary[lst[i]] = bit_code\n",
    "\n",
    "    # find key\n",
    "    output=\"\"\n",
    "    for key, value in dictionary.items():\n",
    "        if value == input_string:\n",
    "            output = key\n",
    "            break\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d96029",
   "metadata": {},
   "source": [
    "## PROCESSING TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f3f88e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01001101011110010010000001110011011001010110001101110010011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "01001101011110010010000001110011011001010110001101110010011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "01001101011110010010000001110011011001010110001101110010011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1101011110010010000001110011011001010110001101110010011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "011110010010000001110011011001010110001101110010011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "10010010000001110011011001010110001101110010011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "0010000001110011011001010110001101110010011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "000001110011011001010110001101110010011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "01110011011001010110001101110010011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "0011011001010110001101110010011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "011001010110001101110010011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "01010110001101110010011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "0110001101110010011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "001101110010011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "01110010011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "0010011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "01010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "0111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "00100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "0000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "00110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "0110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "01100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "0100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "01010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "0010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "01101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "00110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "0011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "00100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "0000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "00010011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "0011000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "000000110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "00110010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "0010001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "001110000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "10000011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "0011010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "010000110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "00110011001100100011010000101110\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "0011001100100011010000101110\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "001100100011010000101110\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "00100011010000101110\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "0011010000101110\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "0011010000101110\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "010000101110\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "00101110\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1110\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "output\n",
      "there was at she one it out to can bad in about about have my soon it is they bad is of they bad watch than in have script of and and no where of of of by in eol every scene it unknown a few to he were like they had my thought was eol great lack a did be they\n",
      "final\n",
      "There was at she one it out to can bad in about about have my soon it is they bad is of they bad watch than in have script of and and no where of of of by in. Every scene it unknown a few to he were like they had my thought was. Great lack a did be they\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "input_text = \"My secret code is: 10284324.\"\n",
    "bitstream = \"\"\n",
    "for c in input_text:\n",
    "    bitstream += bin(ord(c))[2:].zfill(8)\n",
    "print(bitstream)\n",
    "\n",
    "k = 4\n",
    "size_pool = 16\n",
    "gen_output = \"\"\n",
    "gen_sentence = \"\"\n",
    "\n",
    "while bitstream:\n",
    "    while len(bitstream) < k:\n",
    "        bitstream = bitstream + '0'\n",
    "    print(bitstream)\n",
    "    if len(gen_sentence) == 0 or gen_sentence.split()[-1] == 'eol':\n",
    "        if len(gen_sentence) != 0:\n",
    "            gen_output += gen_sentence + \" \"\n",
    "        gen_sentece = \"\"\n",
    "        gen_sentence = np.random.choice(first_top_words)\n",
    "        #print(gen_sentence)\n",
    "        # get random sec word\n",
    "        input_seq = tokenizer.texts_to_sequences([gen_sentence])[0]\n",
    "        input_seq = pad_sequences([input_seq], maxlen=max_input_length)\n",
    "        output_probs = model2.predict(input_seq)[0]\n",
    "        next_word_id = np.random.choice(vocab_size, p=output_probs)\n",
    "        next_word = tokenizer.index_word[next_word_id]\n",
    "        gen_sentence += \" \" + next_word\n",
    "        #print(gen_sentence)\n",
    "    else:\n",
    "        input_seq = tokenizer.texts_to_sequences([gen_sentence])[0]\n",
    "        input_seq = pad_sequences([input_seq], maxlen=max_input_length)\n",
    "        output_probs = model2.predict(input_seq)[0]\n",
    "        '''top_indices = np.argsort(output_probs)[::-1][:8]\n",
    "        top_probs = output_probs[top_indices]\n",
    "        next_word_id = np.random.choice(top_indices, p=top_probs/sum(top_probs))\n",
    "        next_word = tokenizer.index_word[next_word_id]'''\n",
    "        top_words = idx_ziphian_tree(output_probs, tokenizer, size_pool)\n",
    "        next_word = apply_pbt(top_words, k, bitstream[:k])\n",
    "        gen_sentence += \" \" + next_word\n",
    "        \n",
    "        bitstream = bitstream[k:]\n",
    "\n",
    "if len(gen_sentence) != 0:\n",
    "    gen_output += gen_sentence\n",
    "print('output')\n",
    "print(gen_output)\n",
    "\n",
    "sentences = gen_output.split(' eol')\n",
    "\n",
    "# Capitalize the first letter of each sentence and replace 'i' with 'I'\n",
    "output_sentences = []\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.strip()  # remove any leading or trailing white space\n",
    "    sentence = sentence.capitalize()\n",
    "    sentence = sentence.replace(' i ', ' I ')\n",
    "    output_sentences.append(sentence)\n",
    "\n",
    "# Join the sentences back into a single string with '. ' as the separator\n",
    "output_string = '. '.join(output_sentences)\n",
    "\n",
    "print('final')\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79823190",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('movies_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49e40063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "2:  <function is_built_with_cuda at 0x000002A1328629E0>\n",
      "3:  /device:GPU:0\n",
      "4:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print('1: ', tf.config.list_physical_devices('GPU'))\n",
    "print('2: ', tf.test.is_built_with_cuda)\n",
    "print('3: ', tf.test.gpu_device_name())\n",
    "print('4: ', tf.config.get_visible_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf67e3f",
   "metadata": {},
   "source": [
    "## DECODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81fba20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there was at she one it out to can bad in about about have my soon it is they bad is of they bad watch than in have script of and and no where of of of by in eol every scene it unknown a few to he were like they had my thought was eol great lack a did be they\n",
      "-----\n",
      "There was at she one it out to can bad in about about have my soon it is they bad is of they bad watch than in have script of and and no where of of of by in. Every scene it unknown a few to he were like they had my thought was. Great lack a did be they\n",
      "-----\n",
      "there was at she one it out to can bad in about about have my soon it is they bad is of they bad watch than in have script of and and no where of of of by in eol every scene it unknown a few to he were like they had my thought was eol great lack a did be they\n",
      "-----\n",
      "['there was at she one it out to can bad in about about have my soon it is they bad is of they bad watch than in have script of and and no where of of of by in eol', 'every scene it unknown a few to he were like they had my thought was eol', 'great lack a did be they']\n"
     ]
    }
   ],
   "source": [
    "stegotext = output_string\n",
    "original_stego = gen_output\n",
    "print(original_stego)\n",
    "print('-----')\n",
    "print(stegotext)\n",
    "print('-----')\n",
    "stegotext = stegotext.replace(\".\", \" eol\").lower()\n",
    "print(stegotext)\n",
    "print('-----')\n",
    "\n",
    "sentences = []\n",
    "sentence = \"\"\n",
    "for word in stegotext.split():\n",
    "    if \"eol\" in word:\n",
    "        sentence += word\n",
    "        sentences.append(sentence)\n",
    "        sentence = \"\"\n",
    "    else:\n",
    "        sentence += word + \" \"\n",
    "\n",
    "# Add any remaining text as a final sentence\n",
    "if sentence:\n",
    "    sentences.append(sentence.strip())\n",
    "\n",
    "    \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1966720a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n",
      "['my', 'of', 'going', 'there', 'at', 'are', 'into', 'eol', 'they', 'like', 'finally', 'brings', 'such', 'really', 'her', 'modern']\n",
      "at\n",
      "0100\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "['almost', 'eol', 'by', 'of', 'must', \"don't\", 'know', 'that', 'there', 'was', 'at', 'have', 'in', 'she', 'they', 'then']\n",
      "she\n",
      "1101\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "['unknown', 'as', 'out', 'eol', 'were', 'is', 'and', 'one', 'it', 'a', '2', 'of', 'effects', 'least', 'in', 'her']\n",
      "one\n",
      "0111\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "['to', 'and', 'left', 'so', 'goes', \"doesn't\", \"couldn't\", \"he's\", 'a', 'it', 'over', 'was', 'is', 'in', 'will', 'unknown']\n",
      "it\n",
      "1001\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "['as', 'unknown', 'out', 'time', 'least', 'and', \"didn't\", 'world', 'place', 'sense', '2', 'better', 'too', 'were', 'her', 'actor']\n",
      "out\n",
      "0010\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "['to', 'and', 'it', 'a', 'eol', 'you', 'is', 'movie', 'for', 'unknown', 'was', 'i', 'one', 'what', 'all', 'with']\n",
      "to\n",
      "0000\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "['he', 'eol', 'could', 'bad', 'other', 'of', 'have', 'can', 'should', 'and', 'be', \"they're\", 'about', 'look', 'obvious', 'in']\n",
      "can\n",
      "0111\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "['he', 'could', 'be', 'bad', 'other', 'should', 'look', 'eol', 'killer', 'can', 'enough', 'obvious', 'scenes', 'of', 'and', 'close']\n",
      "bad\n",
      "0011\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "['and', 'a', 'said', 'for', 'is', 'but', 'in', 'show', 'lot', 'on', 'film', 'it', 'us', 'any', 'movie', 'their']\n",
      "in\n",
      "0110\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "['eol', 'of', 'that', 'have', 'and', 'about', 'age', 'which', 'they', 'there', 'in', 'i', 'only', 'part', 'acted', 'characters']\n",
      "about\n",
      "0101\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "['eol', 'of', 'and', 'that', 'have', 'i', 'about', 'in', 'a', 'they', 'was', 'which', 'there', 'how', 'at', 'for']\n",
      "about\n",
      "0110\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "['eol', 'and', 'of', 'have', 'that', 'a', 'about', 'in', 'i', 'was', 'they', 'how', 'which', 'its', 'most', 'good']\n",
      "have\n",
      "0011\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "['of', 'get', 'is', 'eol', 'and', 'they', 'line', 'my', 'there', 'never', 'script', 'have', 'working', 'make', 'such', 'about']\n",
      "my\n",
      "0111\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "['thought', 'next', 'soon', 'plays', 'their', 'and', 'show', 'live', 'in', 'is', 'shoot', 'high', 'myself', 'shots', 'english', 'a']\n",
      "soon\n",
      "0010\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "['and', 'is', 'their', 'movie', 'performances', 'a', 'it', 'in', 'for', 'film', 'just', 'you', 'all', 'never', 'plays', 'lot']\n",
      "it\n",
      "0110\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "['as', 'unknown', 'and', 'time', 'out', 'is', 'people', 'were', 'too', \"didn't\", 'her', 'least', 'better', 'world', 'even', 'place']\n",
      "is\n",
      "0101\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "['of', 'are', 'and', 'her', 'like', 'eol', 'was', 'they', 'it', 'sort', 'if', 'new', 'is', 'at', 'up', 'a']\n",
      "they\n",
      "0111\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "['his', 'me', 'be', 'other', 'bad', 'had', 'them', 'were', 'by', 'and', 'too', 'acting', 'people', 'her', 'make', 'eol']\n",
      "bad\n",
      "0100\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "['and', 'a', 'is', 'it', 'in', 'film', 'for', 'just', 'movie', 'with', 'any', 'best', 'lot', 'eol', 'down', 'point']\n",
      "is\n",
      "0010\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "['of', 'eol', 'are', 'and', 'her', 'was', 'like', 'they', 'it', 'new', 'sort', 'at', 'by', 'in', 'if', 'up']\n",
      "of\n",
      "0000\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "['eol', 'different', 'of', 'and', 'in', 'have', 'they', 'a', 'about', 'that', 'good', 'i', 'characters', 'by', 'which', 'most']\n",
      "they\n",
      "0110\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "['his', 'me', 'be', 'bad', 'were', 'by', 'had', 'other', 'them', 'could', 'too', 'and', 'through', 'acting', 'eol', 'people']\n",
      "bad\n",
      "0011\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "['and', 'a', 'is', 'in', 'any', 'it', 'watch', 'lot', 'best', 'for', 'seen', 'point', 'actually', 'film', 'laugh', 'score']\n",
      "watch\n",
      "0110\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "['and', 'that', 'i', 'eol', 'a', 'of', 'for', 'it', 'film', 'even', 'not', 'was', 'in', 'just', 'an', 'than']\n",
      "than\n",
      "1111\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "['eol', 'and', 'of', 'that', 'i', 'was', 'in', 'not', 'good', 'have', 'a', 'from', 'by', 'they', 'most', 'such']\n",
      "in\n",
      "0110\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "['eol', 'of', 'that', 'and', 'have', 'take', 'she', 'i', 'only', 'age', 'part', 'in', 'they', 'by', 'which', 'was']\n",
      "have\n",
      "0100\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "['get', 'of', 'eol', 'is', 'they', 'and', 'script', 'there', 'never', 'my', 'make', 'working', 'line', 'such', 'going', 'at']\n",
      "script\n",
      "0110\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "['and', 'a', 'just', 'is', 'it', 'of', 'film', 'in', 'eol', 'movie', 'for', 'all', 'with', 'you', 'like', 'sea']\n",
      "of\n",
      "0101\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "['eol', 'of', 'and', 'in', 'that', 'have', 'a', 'characters', 'i', 'different', 'they', 'which', 'by', 'its', 'about', 'there']\n",
      "and\n",
      "0010\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "['and', 'eol', 'in', 'of', 'a', 'it', 'i', 'was', 'no', 'way', 'have', 'is', 'this', 'with', 'there', 'from']\n",
      "and\n",
      "0000\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "['eol', 'and', 'in', 'of', 'way', 'i', 'no', 'from', 'have', 'it', 'this', 'are', 'by', 'was', 'unknown', 'there']\n",
      "no\n",
      "0110\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "['amazing', 'at', 'and', 'theater', 'now', 'together', 'will', 'amount', 'made', 'where', 'their', 'worth', 'sight', 'in', 'is', 'to']\n",
      "where\n",
      "1001\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "['from', 'eol', 'one', 'i', 'would', 'not', 'this', 'of', 'been', 'unknown', 'that', 'what', 'his', 'by', 'have', 'in']\n",
      "of\n",
      "0111\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "['eol', 'different', 'in', 'of', 'have', 'and', 'by', 'its', 'which', 'that', 'about', 'scene', 'because', 'a', 'i', 'they']\n",
      "of\n",
      "0011\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "['eol', 'different', 'in', 'of', 'and', 'have', 'its', 'that', 'which', 'by', 'a', 'about', 'scene', 'because', 'i', 'they']\n",
      "of\n",
      "0011\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "['eol', 'different', 'in', 'of', 'and', 'have', 'its', 'that', 'which', 'a', 'by', 'about', 'scene', 'because', 'i', 'they']\n",
      "by\n",
      "1010\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "['eol', 'of', 'in', 'have', 'and', 'a', 'by', 'its', 'about', 'they', 'which', 'that', 'there', 'because', 'different', 'two']\n",
      "in\n",
      "0010\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "['eol', 'of', 'that', 'have', 'and', 'in', 'which', 'about', 'they', 'only', 'a', 'at', 'by', 'there', 'was', 'i']\n",
      "eol\n",
      "0000\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "['unknown', 'as', 'love', 'it', 'to', 'and', 'for', 'this', 'a', 'out', 'was', 'eol', 'his', 'you', 'one', 'if']\n",
      "it\n",
      "0011\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "['as', 'unknown', 'out', 'world', \"didn't\", '2', 'time', 'least', 'were', 'place', 'saw', 'actor', 'reason', 'better', 'too', 'before']\n",
      "unknown\n",
      "0001\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "['unknown', 'and', 'as', 'a', 'out', 'in', 'eol', 'of', 'it', 'were', \"didn't\", 'least', 'before', 'her', '2', 'acting']\n",
      "a\n",
      "0011\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "['few', 'every', 'in', 'guy', 'did', 'and', 'start', 'when', 'up', 'to', 'mention', 'a', 'do', 'but', \"i've\", 'now']\n",
      "few\n",
      "0000\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "['must', 'funny', 'last', 'to', 'ago', 'because', 'will', 'and', 'pacing', 'emotional', 'drawn', 'given', 'thoughts', 'us', 'under', 'passed']\n",
      "to\n",
      "0011\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "['eol', 'could', 'he', 'other', 'bad', 'can', 'have', 'understand', 'that', 'its', 'scenes', 'should', 'same', 'of', 'in', 'be']\n",
      "he\n",
      "0010\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "['unknown', 'as', 'out', 'were', 'and', 'least', 'acting', 'time', 'is', 'make', 'had', \"i'd\", 'it', '2', 'better', 'happy']\n",
      "were\n",
      "0011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step\n",
      "['of', 'it', 'sort', 'are', 'eol', 'new', 'and', 'woman', 'like', 'is', 'definitely', 'her', 'up', 'a', 'an', 'in']\n",
      "like\n",
      "1000\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "['of', 'eol', 'and', 'they', 'have', 'is', 'about', 'one', 'in', 'a', 'it', 'i', 'was', 'would', 'from', 'at']\n",
      "they\n",
      "0011\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "['his', 'me', 'be', 'bad', 'had', 'other', 'were', 'by', 'them', 'too', 'acting', 'pretty', 'should', 'people', 'and', 'look']\n",
      "had\n",
      "0100\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "['is', 'of', 'get', 'my', 'eol', 'they', 'there', 'and', 'have', 'it', 'line', 'going', 'script', 'about', 'make', 'which']\n",
      "my\n",
      "0011\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "['next', 'soon', 'plays', 'thought', 'their', 'and', 'live', 'show', 'is', 'high', 'in', 'shots', 'shoot', 'myself', 'a', 'to']\n",
      "thought\n",
      "0011\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "['and', 'is', 'to', 'was', 'movie', 'a', 'good', 'it', 'eol', 'from', 'film', 'i', 'you', 'one', 'just', 'if']\n",
      "was\n",
      "0011\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "['of', 'and', 'eol', 'new', 'are', 'was', 'her', 'like', 'is', 'it', 'they', 'if', 'sort', 'a', 'in', 'an']\n",
      "eol\n",
      "0010\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "['to', 'and', 'is', 'a', 'it', 'movie', 'unknown', 'for', 'eol', 'you', 'what', 'big', 'man', 'was', 'as', 'all']\n",
      "a\n",
      "0011\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "['few', 'every', 'when', 'guy', 'did', 'but', 'in', 'on', 'where', 'start', 'and', 'mention', 'also', 'up', 'do', 'see']\n",
      "did\n",
      "0100\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "['are', 'and', 'be', 'for', 'this', 'was', 'a', 'i', 'movie', 'is', 'he', 'eol', 'of', 'not', 'film', 'like']\n",
      "be\n",
      "0010\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "['of', 'and', 'make', 'eol', 'like', 'is', 'it', 'when', 'script', 'another', 'shows', 'her', 'an', 'up', 'they', 'a']\n",
      "they\n",
      "1110\n",
      "01001101011110010010000001110011011001010110001101110010011001010111010000100000011000110110111101100100011001010010000001101001011100110011101000100000001100010011000000110010001110000011010000110011001100100011010000101110\n",
      "56\n",
      "My secret code is: 10284324.\n"
     ]
    }
   ],
   "source": [
    "output_bitstream = \"\"\n",
    "i = 0\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    sentence_random = ' '.join(words[:2])\n",
    "    sentence_main = ' '.join(words[2:])\n",
    "    #print(sentence_random)\n",
    "    #print(sentence_main)\n",
    "    \n",
    "    words_main = sentence_main.split()\n",
    "    stego_sentence = sentence_random\n",
    "    for word in words_main:\n",
    "        input_seq = tokenizer.texts_to_sequences([stego_sentence])[0]\n",
    "        input_seq = pad_sequences([input_seq], maxlen=max_input_length)\n",
    "        output_probs = model2.predict(input_seq)[0]\n",
    "        top_words = idx_ziphian_tree(output_probs, tokenizer, size_pool)\n",
    "        print(top_words)\n",
    "        print(word)\n",
    "        idx_target = get_index(top_words, word)\n",
    "        #print(idx_target)\n",
    "        word_bits = int_to_binstr(idx_target, k)\n",
    "        print(word_bits)\n",
    "        output_bitstream += word_bits\n",
    "        #print(output_bitstream)\n",
    "        stego_sentence += ' ' + word\n",
    "        i += 1\n",
    "\n",
    "print(output_bitstream)\n",
    "print(i)\n",
    "\n",
    "# Split bitstream into 8-bit chunks\n",
    "chunks = [output_bitstream[i:i+8] for i in range(0, len(output_bitstream), 8)]\n",
    "\n",
    "# Convert each chunk to its corresponding ASCII character\n",
    "string = \"\"\n",
    "for chunk in chunks:\n",
    "    string += chr(int(chunk, 2))\n",
    "\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aae992a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00010\n"
     ]
    }
   ],
   "source": [
    "def get_index(lst, val):\n",
    "    try:\n",
    "        return lst.index(val)\n",
    "    except ValueError:\n",
    "        return -1\n",
    "\n",
    "def int_to_binstr(a, k):\n",
    "    binstr = bin(a)[2:]\n",
    "    return \"0\" * (k - len(binstr)) + binstr\n",
    "\n",
    "print(int_to_binstr(2, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
